---
title: "Lab5"
author: "Amanda Herbst"
date: "2024-05-17"
output: html_document
---

```{r, include = FALSE}
library(tidytext)
library(tidyverse)
library(widyr) 
library(irlba)
library(broom) 
library(textdata)
library(ggplot2)
library(dplyr)
library(here)
library(LexisNexisTools)
```


### Lab 5 Assignment

#### Train Your Own Embeddings

Read in data
```{r read-data, message=FALSE, warning=FALSE}
setwd(here("data/offshore_wind_articles")) #where the .docxs live
post_files <- list.files(pattern = ".docx", path = getwd(),
                      full.names = TRUE, recursive = TRUE, ignore.case = TRUE)

# read list of file paths into an lnt object
wind_lnt <- lnt_read(post_files, remove_cover = FALSE) 

# @ shows the objects in each "slot" of the S4 object
meta_df <- wind_lnt@meta
articles_df <- wind_lnt@articles
paragraphs_df <- wind_lnt@paragraphs

# create new dataframe with pieces of what we want to use from above
wind_df <- tibble(Date = meta_df$Date,
               Headline = meta_df$Headline,
               ID = articles_df$ID,
               Text = articles_df$Article)
```
1.  Using the data from your Nexis Uni query from Week 2, create a set of word embeddings. To do this, you'll essentially need to recreate the steps in today's example down through the chunk named "pmi". 

Unigram probabilities
```{r unigrams}
unigram_probs <- wind_df %>% 
  unnest_tokens(word, Text) %>% 
  anti_join(stop_words, by = "word") %>% 
  count(word, sort = TRUE) %>% # count frequency of each word that appears
  mutate(p = n/sum(n))

unigram_probs
```

Skipgrams
```{r make-skipgrams}
skipgrams <- wind_df %>% 
  unnest_tokens(ngram, Text, token = "ngrams", n = 5) %>% 
  mutate(ngramID = row_number()) %>% 
  tidyr::unite(skipgramID, ID, ngramID) %>% 
  unnest_tokens(word, ngram) %>% 
  anti_join(stop_words, by = "word")
 
skipgrams 
```

```{r pairwise_count}
skipgram_probs  <- skipgrams %>% 
  # within each skipgram ID, counts up pairs of words
  pairwise_count(item = word, feature = skipgramID, upper = FALSE) %>% # reduce data with diag = F
  mutate(p = n/sum(n)) # create probabilities of a given pair

skipgram_probs
```

Normalize Probabilities
```{r norm-prob}
normalized_probs <- skipgram_probs %>% 
  rename(word1 = item1, word2 = item2) %>% 
  left_join(unigram_probs %>% 
              select(word1 = word, p1 = p), by = "word1") %>% 
  left_join(unigram_probs %>% 
              select(word2 = word, p2 = p), by = "word2") %>% 
  mutate(p_together = p/p1/p2)

normalized_probs[1:10,]
```

```{r pmi}
pmi_matrix <- normalized_probs %>% 
  mutate(pmi = log10(p_together)) %>% 
  cast_sparse(word1, word2, pmi)
```

2.  Think of 3 important words in your data set. Calculate and plot the 10 most semantically similar words for each of them. Identify and interpret any interesting or surprising results.
**energy, environment, renewable**

Partial Decomposition
```{r svd}
# references all elements of our matrix, replace all NA values with 0
pmi_matrix@x[is.na(pmi_matrix@x)] <- 0

# factorize our matrix down to 100 dimensions
pmi_svd <- irlba::irlba(pmi_matrix, 100, verbose = FALSE)

word_vectors <- pmi_svd$u

rownames(word_vectors) <- rownames(pmi_matrix)
```

Function to estimate semantically similar words
```{r syn_function}
search_synonyms <- function(word_vectors, selected_vector, original_word) {
  
  dat <- word_vectors %*% selected_vector # dot product of matrix and our single vector
  similarities <- as.data.frame(dat) %>% 
    tibble(token = rownames(dat), similarity = dat[,1]) %>% 
    filter(token != original_word) %>% 
    arrange(desc(similarity)) %>%  # which words are most similar to the provided word
    select(token, similarity)

  return(similarities)
}
```


```{r find-synonyms}
energy <- search_synonyms(word_vectors, word_vectors["energy",], "energy")

ocean <- search_synonyms(word_vectors, word_vectors["ocean",], "ocean")

renewable <- search_synonyms(word_vectors, word_vectors["renewable",], "renewable")
```

```{r plot-synonyms}
energy %>% 
  mutate(selected = "energy") %>% 
  bind_rows(ocean %>% 
              mutate(selected = "ocean")) %>% 
   bind_rows(renewable %>% 
              mutate(selected = "renewable")) %>% 
  group_by(selected) %>% 
  top_n(10, similarity) %>% 
  mutate(token = reorder(token, similarity)) %>% 
  ggplot(aes(token, similarity, fill = selected)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~selected, scales = "free") +
  coord_flip() +
  labs(x = NULL, title = "Which word vectors are most similar to energy, environemnt or renewable?") +
  theme_bw()
```
3.  Assemble 3 word math equations that you think could be useful or interesting for exploring the meaning of key words or ideas in your data set.
- **renewable energy**
- **environmental impacts**
- **climate change**
```{r word-math}
renewable_energy <- word_vectors["renewable",] + word_vectors["energy",]
head(search_synonyms(word_vectors, selected_vector = renewable_energy, ""))

environmental_impact <- word_vectors["environmental",] + word_vectors["impacts",]
head(search_synonyms(word_vectors, environmental_impact, ""))

MPA <- word_vectors["marine",] + word_vectors["protected",] + word_vectors["tribal",] 
head(search_synonyms(word_vectors, MPA, ""))
```


#### Pretrained Embeddings

4.  Following the example in the SMLTR text (section 5.4), create a set of 100-dimensional GloVe word embeddings. These embeddings were trained by researchers at Stanford on 6 billion tokens from Wikipedia entries.

Note: The embeddings .zip file is very large. You may have to increase your global timeout setting to download, ex: options(timeout=100)
```{r GloVe-embeddings}
glove6b <- embedding_glove6b(dimensions = 100)
```

Don't have to do partial decomposition because it is already 100 dimensions!
```{r tidy-GloVe}
tidy_glove <- glove6b %>%
  pivot_longer(contains("d"),
               names_to = "dimension") %>%
  rename(item1 = token)

tidy_glove
```

5.  Test them out with the canonical word math equation on the GloVe embeddings: "berlin" - "germany" + "france" = ?

Hint: you'll need to convert the GloVe dataframe to a matrix and set the row names in order to use our synonym function.

```{r}
# convert to matrix and set row names as the first column
glove_matrix <- glove6b %>% 
  tibble::column_to_rownames(var = "token") %>% 
  as.matrix()

# test word math
word_math_test <- glove_matrix["berlin",] - glove_matrix["germany",] + glove_matrix["france",]
search_synonyms(glove_matrix, word_math_test, "")
```


6.  Recreate parts 2 and 3 above using the the GloVe embeddings in place of the ones you made. How do they compare? What are the implications for applications of these embeddings?

```{r glove-synonyms}
energy <- search_synonyms(glove_matrix, glove_matrix["energy",], "energy")

ocean <- search_synonyms(glove_matrix, glove_matrix["ocean",], "ocean")

renewable <- search_synonyms(glove_matrix, glove_matrix["renewable",], "renewable")
```

```{r plot-glove-synonyms}
energy %>% 
  mutate(selected = "energy") %>% 
  bind_rows(ocean %>% 
              mutate(selected = "ocean")) %>% 
   bind_rows(renewable %>% 
              mutate(selected = "renewable")) %>% 
  group_by(selected) %>% 
  top_n(10, similarity) %>% 
  mutate(token = reorder(token, similarity)) %>% 
  ggplot(aes(token, similarity, fill = selected)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~selected, scales = "free") +
  coord_flip() +
  labs(x = NULL, title = "Which word vectors are most similar to energy, environment or renewable?") +
  theme_bw()
```

```{r}
renewable_energy <- glove_matrix["renewable",] + glove_matrix["energy",]
head(search_synonyms(glove_matrix, selected_vector = renewable_energy, ""))

environmental_impact <- glove_matrix["environmental",] + glove_matrix["impacts",]
head(search_synonyms(glove_matrix, environmental_impact, ""))

MPA <- glove_matrix["marine",] + glove_matrix["protected",] + glove_matrix["tribal",] 
head(search_synonyms(glove_matrix, MPA, ""))
```

**The GloVe embeddings 
